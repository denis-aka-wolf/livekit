version: '3.8'

services:
  redis:
    image: redis:7-alpine
    network_mode: host
    restart: always
    volumes:
      - redis_data:/data

  livekit:
    image: livekit/livekit-server:latest
    network_mode: host
    restart: always
    depends_on:
      - redis
    env_file:
      - .env
    environment:
      # Установите свои ключи в переменных окружения
      # LIVEKIT_KEYS: "your_api_key: your_api_secret"
      LIVEKIT_KEYS: "${LIVEKIT_API_KEY:-YOUR_API_KEY_HERE}: ${LIVEKIT_API_SECRET:-YOUR_API_SECRET_HERE}"
      LIVEKIT_REDIS_ADDRESS: "localhost:6379"
      LIVEKIT_DEV: "true"
    volumes:
      - livekit_data:/var/lib/livekit

  sip:
    image: livekit/sip:latest
    network_mode: host
    restart: always
    env_file:
      - .env
    environment:
      SIP_CONFIG_BODY: |
        api_key: '${LIVEKIT_API_KEY:-YOUR_API_KEY_HERE}'
        api_secret: '${LIVEKIT_API_SECRET:-YOUR_API_SECRET_HERE}'
        ws_url: 'ws://localhost:7880'
        redis:
          address: 'localhost:6379'
        sip_port: 5060
        udp_port: 5060
        rtp_port_range: 10000-20000
        logging:
          level: info

  llama_cpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    network_mode: host
    command:
      - --host
      - 0.0.0.0
      - --port
      - "11434"
      - --hf-repo
      - "${LLAMA_HF_REPO:-unsloth/Qwen3-4B-Instruct-2507-GGUF}"
      - --alias
      - "${LLAMA_MODEL_ALIAS:-qwen3-4b}"
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-16384}"
    ports:
      - "${LLAMA_HOST_PORT:-11436}:11434"
    volumes:
      - ./inference/llama/models:/models
    environment:
      - XDG_CACHE_HOME=/models
      - HF_HOME=/models
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:11434/v1/models > /dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30          
      

volumes:
  redis_data:
  livekit_data:
