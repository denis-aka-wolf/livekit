import asyncio
import logging
import os
import sys
from typing import Any, Dict

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ elaina_tts
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'elaina_tts'))

from livekit import rtc
from livekit.agents import metrics, MetricsCollectedEvent
from livekit.agents.llm import ChatMessage
from livekit.plugins import openai, silero
from elaina_tts.elaina_tts import ElainaTTS

logger = logging.getLogger("elaina-inbound-worker")
logger.setLevel(logging.INFO)


def setup_vad(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Voice Activity Detection"""
    return silero.VAD.load(
        min_speech_duration=config["min_speech_duration"],
        min_silence_duration=config["min_silence_duration"],
        prefix_padding_duration=config["prefix_padding_duration"],
    )


def setup_stt(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Speech-to-Text"""
    # –ü–æ–ª—É—á–∞–µ–º URL –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω –ª–æ–∫–∞–ª—å–Ω—ã–º
    base_url = config.get("base_url", "")
    logger.info(f"Setting up STT with config: base_url={base_url}, model={config.get('model')}")
    
    # –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ
    if base_url and ("localhost" in base_url or "127.0.0.1" in base_url or "0.0.0.0" in base_url):
        logger.info(f"Using local STT service: {base_url}")
        # –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö STT —Å–µ—Ä–≤–∏—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, whisper-asr-server) —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        return openai.STT(
            base_url=config["base_url"],
            model=config.get("model", "Systran/faster-whisper-small"),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –∞ –Ω–µ getenv –Ω–∞–ø—Ä—è–º—É—é
            api_key=config.get("api_key", "no-key-needed"),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–ª—é—á –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
            language=config.get("language", "ru"),
        )
    else:
        logger.info(f"Using external STT service: {base_url}")
        # –ï—Å–ª–∏ —ç—Ç–æ –≤–Ω–µ—à–Ω–∏–π API OpenAI
        return openai.STT(
            base_url=config["base_url"],
            model=config["model"],
            api_key=config["api_key"],
            language=config["language"],
        )


def setup_tts(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Text-to-Speech"""
    return ElainaTTS(
        speaker=config["speaker"],
        sample_rate=config["sample_rate"],
        num_channels=config["num_channels"]
    )


def setup_llm(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Large Language Model"""
    from livekit.plugins.openai import LLM
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –ª–æ–∫–∞–ª—å–Ω—ã–º, –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    base_url = config.get("base_url", "")
    logger.info(f"Setting up LLM with config: base_url={base_url}, model={config.get('model')}")
    
    if base_url and ("localhost" in base_url or "127.0.0.1" in base_url or "0.0.0.0" in base_url):
        logger.info(f"Using local LLM service: {base_url}")
        # –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM (–Ω–∞–ø—Ä–∏–º–µ—Ä, Ollama, llama.cpp) —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –∫–æ–¥–∞
        return LLM(
            base_url=config["base_url"],
            model=config["model"],
            api_key=config.get("api_key", "no-key-needed"),
            timeout=config.get("timeout", 30.0),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            max_retries=config["max_retries"],
        )
    else:
        logger.info(f"Using external LLM service: {base_url}")
        # –î–ª—è –≤–Ω–µ—à–Ω–µ–≥–æ OpenAI API
        return LLM(
            base_url=config["base_url"],
            model=config["model"],
            api_key=config["api_key"],
            timeout=config["timeout"],
            max_retries=config["max_retries"],
        )


def setup_session_config(session_config: Dict[str, Any], stt, tts, llm, vad):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏ –∞–≥–µ–Ω—Ç–∞"""
    return {
        # turn_detection=MultilingualModel(), # –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ —Ñ—Ä–∞–∑—ã
        "vad": vad,  # –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —Ä–µ—á–∏ –≤ –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–µ
        # –ì–ª–∞–≤–Ω—ã–µ —Ä—É—á–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏:
        "min_endpointing_delay": session_config["min_endpointing_delay"],
        "min_interruption_words": session_config["min_interruption_words"],
        "stt": stt,
        "tts": tts,
        "llm": llm,
    }


def setup_metrics_handler(session):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –º–µ—Ç—Ä–∏–∫"""
    @session.on("metrics_collected")
    def _on_metrics_collected(ev: MetricsCollectedEvent):
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
        #metrics.log_metrics(ev.metrics)
        
        # –¢–∞–∫–∂–µ –º–æ–∂–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ
        metric_type = type(ev.metrics).__name__
        if metric_type == "EOUMetrics":
            logger.info(f"[VAD] –ö–æ–Ω–µ—Ü —Ñ—Ä–∞–∑—ã –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑: {ev.metrics.end_of_utterance_delay:.2f}—Å")
        elif metric_type == "STTMetrics":
            logger.info(f"[STT] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –∑–∞: {ev.metrics.duration:.2f}—Å")
        elif metric_type == "LLMMetrics":
            logger.info(f"[LLM] –í—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ (TTFT): {ev.metrics.ttft:.2f}—Å")
            logger.info(f"[LLM] –û–±—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {ev.metrics.duration:.2f}—Å")
        elif metric_type == "TTSMetrics":
            logger.info(f"[TTS] –í—Ä–µ–º—è –¥–æ –Ω–∞—á–∞–ª–∞ –∑–≤—É–∫–∞ (TTFB): {ev.metrics.ttfb:.2f}—Å")


async def warmup_llm(llm, phone_number: str, client_name: str):
    """–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ LLM"""
    try:
        from .prompt_processor import load_and_process_prompt
        warmup_prompt = load_and_process_prompt(phone_number, client_name)
        logger.info("üî• –ù–∞—á–∏–Ω–∞—é —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ–≤ LLM...")
        # –ó–∞–ø—É—Å–∫–∞–µ–º —á–∞—Ç
        chat_stream = llm.chat(
            history=[ChatMessage(role="system", content=[{"type": "text", "text": warmup_prompt}]),
                     ChatMessage(role="user", content=[{"type": "text", "text": "–ü—Ä–∏–≤–µ—Ç"}])], # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–∏—Ç–∞—Ü–∏—é —é–∑–µ—Ä–∞
            temperature=0.7
        )
        # –í–ê–ñ–ù–û: –Ω—É–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ –ø–æ—Ç–æ–∫–∞, 
        # —á—Ç–æ–±—ã llama_cpp –Ω–∞—á–∞–ª–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
        async for chunk in chat_stream:
            # –ù–∞–º –Ω–µ –Ω—É–∂–µ–Ω —Ç–µ–∫—Å—Ç, –Ω–∞–º –Ω—É–∂–µ–Ω —Å–∞–º —Ñ–∞–∫—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
            break 
        logger.info("‚úÖ Prompt warmup completed (–∫—ç—à —Å–æ–∑–¥–∞–Ω)")
    except Exception as e:
        logger.warning(f"Prompt warmup failed: {e}")