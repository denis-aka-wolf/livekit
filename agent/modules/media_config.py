import asyncio
import logging
import os
import sys
from typing import Any, Dict

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ elaina_tts
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'elaina_tts'))

from livekit import rtc
from livekit.agents import metrics, MetricsCollectedEvent
from livekit.agents.llm import ChatMessage
from livekit.plugins import openai, silero
from elaina_tts.elaina_tts import ElainaTTS

logger = logging.getLogger("elaina-inbound-worker")
logger.setLevel(logging.INFO)


def setup_vad(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Voice Activity Detection"""
    return silero.VAD.load(
        min_speech_duration=config["min_speech_duration"],
        min_silence_duration=config["min_silence_duration"],
        prefix_padding_duration=config["prefix_padding_duration"],
    )


def setup_stt(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Speech-to-Text"""
    return openai.STT(
        base_url=config["base_url"],
        model=config["model"],
        api_key=config["api_key"],
        language=config["language"],
    )


def setup_tts(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Text-to-Speech"""
    return ElainaTTS(
        speaker=config["speaker"],
        sample_rate=config["sample_rate"],
        num_channels=config["num_channels"]
    )


def setup_llm(config: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Large Language Model"""
    return openai.LLM(
        base_url=config["base_url"],
        model=config["model"],
        api_key=config["api_key"],
        timeout=config["timeout"],
        max_retries=config["max_retries"],
    )


def setup_session_config(session_config: Dict[str, Any], stt, tts, llm, vad):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏ –∞–≥–µ–Ω—Ç–∞"""
    return {
        # turn_detection=MultilingualModel(), # –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ —Ñ—Ä–∞–∑—ã
        "vad": vad,  # –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —Ä–µ—á–∏ –≤ –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–µ
        # –ì–ª–∞–≤–Ω—ã–µ —Ä—É—á–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏:
        "min_endpointing_delay": session_config["min_endpointing_delay"],
        "min_interruption_words": session_config["min_interruption_words"],
        "stt": stt,
        "tts": tts,
        "llm": llm,
    }


def setup_metrics_handler(session):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –º–µ—Ç—Ä–∏–∫"""
    @session.on("metrics_collected")
    def _on_metrics_collected(ev: MetricsCollectedEvent):
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
        #metrics.log_metrics(ev.metrics)
        
        # –¢–∞–∫–∂–µ –º–æ–∂–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ
        metric_type = type(ev.metrics).__name__
        if metric_type == "EOUMetrics":
            logger.info(f"[VAD] –ö–æ–Ω–µ—Ü —Ñ—Ä–∞–∑—ã –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑: {ev.metrics.end_of_utterance_delay:.2f}—Å")
        elif metric_type == "STTMetrics":
            logger.info(f"[STT] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –∑–∞: {ev.metrics.duration:.2f}—Å")
        elif metric_type == "LLMMetrics":
            logger.info(f"[LLM] –í—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ (TTFT): {ev.metrics.ttft:.2f}—Å")
            logger.info(f"[LLM] –û–±—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {ev.metrics.duration:.2f}—Å")
        elif metric_type == "TTSMetrics":
            logger.info(f"[TTS] –í—Ä–µ–º—è –¥–æ –Ω–∞—á–∞–ª–∞ –∑–≤—É–∫–∞ (TTFB): {ev.metrics.ttfb:.2f}—Å")


async def warmup_llm(llm, phone_number: str, client_name: str):
    """–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ LLM"""
    try:
        from .prompt_processor import load_and_process_prompt
        warmup_prompt = load_and_process_prompt(phone_number, client_name)
        logger.info("üî• –ù–∞—á–∏–Ω–∞—é —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ–≤ LLM...")
        # –ó–∞–ø—É—Å–∫–∞–µ–º —á–∞—Ç
        chat_stream = llm.chat(
            history=[ChatMessage(role="system", content=[{"type": "text", "text": warmup_prompt}]),
                     ChatMessage(role="user", content=[{"type": "text", "text": "–ü—Ä–∏–≤–µ—Ç"}])], # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–∏—Ç–∞—Ü–∏—é —é–∑–µ—Ä–∞
            temperature=0.7
        )
        # –í–ê–ñ–ù–û: –Ω—É–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ –ø–æ—Ç–æ–∫–∞, 
        # —á—Ç–æ–±—ã llama_cpp –Ω–∞—á–∞–ª–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
        async for chunk in chat_stream:
            # –ù–∞–º –Ω–µ –Ω—É–∂–µ–Ω —Ç–µ–∫—Å—Ç, –Ω–∞–º –Ω—É–∂–µ–Ω —Å–∞–º —Ñ–∞–∫—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
            break 
        logger.info("‚úÖ Prompt warmup completed (–∫—ç—à —Å–æ–∑–¥–∞–Ω)")
    except Exception as e:
        logger.warning(f"Prompt warmup failed: {e}")