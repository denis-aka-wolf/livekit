services:
  redis:
    image: redis:7-alpine
    container_name: redis
    network_mode: host
    restart: always
    command: ["redis-server", "/usr/local/etc/redis/redis.conf"]
    volumes:
      - /srv/livekit/data/redis:/data
      - /srv/livekit/redis.conf:/usr/local/etc/redis/redis.conf

  livekit:
    image: livekit/livekit-server:v1.9.8
    container_name: livekit
    network_mode: host
    restart: always
    depends_on:
      - redis
    env_file:
      - .env
    environment:
      LIVEKIT_KEYS: "${LIVEKIT_API_KEY:-YOUR_API_KEY_HERE}: ${LIVEKIT_API_SECRET:-YOUR_API_SECRET_HERE}"
      LIVEKIT_REDIS_ADDRESS: "127.0.0.1:6379"
      LIVEKIT_DEV: "false"
    entrypoint: ["/livekit-server", "--config", "/livekit.yaml"]
    volumes:
      - /srv/livekit/data/livekit:/var/lib/livekit
      - /srv/livekit/livekit.yaml:/livekit.yaml

  sip:
    image: livekit/sip:latest
    container_name: sip
    network_mode: host
    restart: always
    env_file:
      - .env
    environment:
      SIP_CONFIG_BODY: |
        api_key: '${LIVEKIT_API_KEY:-YOUR_API_KEY_HERE}'
        api_secret: '${LIVEKIT_API_SECRET:-YOUR_API_SECRET_HERE}'
        ws_url: 'ws://localhost:7880'
        redis:
          address: '127.0.0.1:6379'
        sip_port: 5060
        udp_port: 5060
        rtp_port_range: 10000-20000
        use_external_ip: true
        logging:
          level: info
    volumes:
      - /srv/livekit/logs:/var/log/livekit

  llama_cpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama_cpp
    network_mode: host
    restart: unless-stopped
    command:
      - --host
      - 0.0.0.0
      - --port
      - "11434"
      - --hf-repo
      - "unsloth/Qwen3-4B-Instruct-2507-GGUF" #- "${LLAMA_HF_REPO:-unsloth/Qwen3-4B-Instruct-2507-GGUF}"
      - --hf-file
      - "Qwen3-4B-Instruct-2507-Q4_K_M.gguf" # Явно просим легкую версию
      - --alias
      - "${LLAMA_MODEL_ALIAS:-qwen3-4b}"
      - --ctx-size
      - "4096" #"${LLAMA_CTX_SIZE:-16384}"
      - --threads
      - "8"  # Потому что команда lscpu выводит Core(s) per socket:   8
      - --n-gpu-layers
      - "0"  # Отключаем GPU для стабильности CPU-обработки
      - --parallel
      - "1" # Количество параллельных запросов
      - --batch-size
      - "512" 
      - --mlock   # Чтобы модель не улетала в SWAP
      - --log-disable  # Отключаем логирование для улучшения производительности
    volumes:
      - /srv/livekit/inference/llama/models:/models
    environment:
      - XDG_CACHE_HOME=/models
      - HF_HOME=/models
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:11434/v1/models > /dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30

  kokoro:
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest
    container_name: kokoro
    restart: unless-stopped
    ports:
      - "8880:8880"

  whisper:
    build:
      context: ./inference/whisper
    container_name: whisper
    environment:
      - VOXBOX_HF_REPO_ID=Systran/faster-whisper-tiny
      - VOXBOX_DEVICE=cpu
      - VOXBOX_COMPUTE_TYPE=int8
      - OMP_NUM_THREADS=8
      - CT2_THREAD_COUNT=8
    volumes:
      - /srv/livekit/whisper:/data
    ports:
      - "11435:80"

  nginx:
    image: nginx:alpine
    container_name: nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /srv/livekit/nginx/conf.d:/etc/nginx/conf.d
      - /srv/livekit/nginx/html:/usr/share/nginx/html
      # Монтируем сертификаты Let’s Encrypt напрямую в контейнер
      - /etc/letsencrypt/live/brainsync.ru/fullchain.pem:/etc/nginx/certs/brainsync.crt:ro
      - /etc/letsencrypt/live/brainsync.ru/privkey.pem:/etc/nginx/certs/brainsync.key:ro
